{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4fe60a-7f72-4581-977e-185fed0bfcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9584758-fade-43d5-b23e-f41db074b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, q, k, v, d_k, dropout=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "    def forward(self, q, k, v):\n",
    "        bs = q.size(0)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        scores = self.attention(q, k, v, self.d_k, self.dropout)\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d725c663-d8f5-45e2-a6ed-96e50c1d68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ddbd73-059b-4da2-8a2d-bb39dd0045b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormLayer(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d9272a-aa90-4139-b7d4-265e32635516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayers(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = NormLayer(d_model)\n",
    "        self.norm_2 = NormLayer(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2, x2, x2))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a9128-06b5-4d73-a792-7e3351634e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, v_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = nn.Embedding(v_size, d_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de4d2d1c-c477-4256-b96d-ad4156552207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
    "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
    "    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
    "    omega = 1.0 / (temperature ** omega)\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, channels, dim):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        \n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0\n",
    "        \n",
    "        self.patch_height = patch_height\n",
    "        self.patch_width = patch_width\n",
    "        self.num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        \n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # 重新排列为 patches: [B, num_patches, patch_dim]\n",
    "        x = x.unfold(2, self.patch_height, self.patch_height)  # [B, C, H//pH, W, pH]\n",
    "        x = x.unfold(3, self.patch_width, self.patch_width)    # [B, C, H//pH, W//pW, pH, pW]\n",
    "        x = x.contiguous().view(B, C, -1, self.patch_height * self.patch_width)  # [B, C, num_patches, pH*pW]\n",
    "        x = x.permute(0, 2, 1, 3)  # [B, num_patches, C, pH*pW]\n",
    "        x = x.contiguous().view(B, -1, C * self.patch_height * self.patch_width)  # [B, num_patches, patch_dim]\n",
    "        \n",
    "        return self.proj(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias=False),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, _ = x.shape\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = qkv\n",
    "        \n",
    "        # 重塑为多头: [B, N, heads, dim_head] -> [B, heads, N, dim_head]\n",
    "        q = q.view(B, N, self.heads, -1).transpose(1, 2)\n",
    "        k = k.view(B, N, self.heads, -1).transpose(1, 2)\n",
    "        v = v.view(B, N, self.heads, -1).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = torch.softmax(dots, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        # 重新组合: [B, heads, N, dim_head] -> [B, N, heads*dim_head]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, N, -1)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout=dropout)\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, \n",
    "                 heads, mlp_dim, channels=3, dim_head=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        \n",
    "        self.to_patch_embedding = PatchEmbedding(image_size, patch_size, channels, dim)\n",
    "        \n",
    "        self.pos_embedding = posemb_sincos_2d(\n",
    "            h=image_height // patch_height,\n",
    "            w=image_width // patch_width,\n",
    "            dim=dim,\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.linear_head = nn.Linear(dim, num_classes)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        device = img.device\n",
    "        x = self.to_patch_embedding(img)\n",
    "        x += self.pos_embedding.to(device, dtype=x.dtype)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f5bb7-d0c3-4a85-8e51-0ced3be62100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bevnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
